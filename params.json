{"name":"Predictionassignment","tagline":"","body":"---\r\ntitle: \"Practical Machine Learning Course Project\"\r\nauthor: \"Victoria O.\"\r\ndate: \"20/07/2020\"\r\noutput: html_document\r\n---\r\n\r\n## Introduction\r\nOne thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.\r\n\r\nThe goal of this project is to predict the manner in which they did the exercise. This is the \"classe\" variable in the training set. The prediction model developed will be used to predict 20 different test cases.\r\n\r\n## Load Packages\r\n```{r }\r\nlibrary(caret)\r\nlibrary(randomForest)\r\nlibrary(lattice)\r\n```\r\n\r\n## Load data \r\nLoading training and testing data sets. \r\n```{r }\r\ntraining_data <- read.csv(\"pml-training.csv\", header=TRUE, sep=\",\",stringsAsFactors=FALSE)\r\ntesting_data <- read.csv(\"pml-testing.csv\", header=TRUE, sep=\",\",stringsAsFactors=FALSE)\r\nsummary(training_data$var_total_accel_belt)\r\n```\r\nThere are 19622 observations, consisting of 160 columns\r\n## Data preprocessing\r\n\r\nThere are many variables in the dataset contain invalid values such as NAâ€™s and blanks. These variables with large amount of invalid values are removed. \r\n\r\n```{r }\r\ndataTidy <- training_data[,-c(grep(\"^amplitude|^kurtosis|^skewness|^avg|^cvtd_timestamp|^max|^min|^new_window|^raw_timestamp|^stddev|^var|^user_name|X\",names(training_data)))]\r\n\r\npaste(\"Complete Cases:\")\r\ntable(complete.cases(dataTidy))\r\n```\r\n### Data Split\r\nthe tidy data is split into two sets, 60% for training and 40% for testing\r\n\r\n```{r }\r\nset.seed(39)\r\ninTrain <- createDataPartition(y=dataTidy$classe, p=0.6,list=FALSE)\r\ndataTidyTrain <- dataTidy[inTrain,]\r\ndataTidyTest <- dataTidy[-inTrain,]\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n## Model training and tuning \r\n\r\nThe RandomForest and Gradient Boosting model is built. A random forest model is one of the best prediction models. It can reduce training variance and sensitivity to overfitting. \r\n\r\nIt is determined that this is a classification problem and the aim of the comparison is to discover which algorithm suits the data better.The RandomForest and Gradient Boosting algorithms are selected for comparison based on the accuracy these algorithms can achieve in classification. The Kappa metric is selected as the comparison criteria. To reduce the risk of overfitting, a 10-fold cross validation is employed during model building. \r\n\r\n```{r }\r\n# Model tuning and training\r\nset.seed(39)\r\n# k-fold validation - 10-fold validation, use kappa as metric\r\nfitControl <- trainControl(method = \"cv\",\r\n                           number = 10)\r\ngbmFit <- train(classe~., data=dataTidyTrain, method=\"gbm\", metric=\"Kappa\", trControl=fitControl,verbose=FALSE)\r\n```\r\n\r\n```{r }\r\nrfFit <- train(classe~.,data=dataTidyTrain,method=\"rf\", metric=\"Kappa\", trControl=fitControl)\r\n```\r\n\r\n## Model Selection\r\nThe models are  compared. Based on the plot, RandomForest algorithm fares better than the Gradient Boosting algorithm for this dataset, achieving a Kappa mean value of 0.996. It can also be seen that the RandomForest algorithm also displays less spread than Gradient Boosting. Therefore, the RandomForest model is selected for this dataset.\r\n```{r }\r\nrValues <- resamples(list(rf=rfFit,gbm=gbmFit))\r\nsummary(rValues)\r\nbwplot(rValues,metric=\"Kappa\",main=\"RandomForest (rf) vs Gradient Boosting (gbm)\")\r\n```\r\n\r\n\r\n## Cross Validation\r\nWe measure the accuracy of our model by using our training and  cross validation sets. \r\n```{r }\r\nrfFit\r\n```\r\n\r\n###  Accuracy\r\n\r\nThe confusionMatrix function in the Caret package to validate the selected model with the dataTidyTest test set. The corresponding statistics and error rates are shown..\r\n\r\n```{r }\r\nconfusionMatrix(dataTidyTest$classe, predict(rfFit,dataTidyTest))\r\n```\r\nFrom the result, the selected Model performs at a Kappa value of 0.995, with an accuracy of 99.6%.\r\n\r\n\r\n\r\n## Test Set Prediction Results\r\n\r\nThe selected model is used to predict the classification of the testing set provided.\r\n```{r }\r\nresults <- predict(rfFit,newdata=testing_data)\r\nprint(as.data.frame(results))\r\n```\r\n\r\n\r\n## Conclusion\r\nThe Random Forest Model provides very good prediction accuracy of the manner in which participants did the weight lifting as measured with accelerometers.\r\n\r\n\r\n\r\n\r\n## References\r\n\r\n[1] [Data Source](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har)\r\n\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}